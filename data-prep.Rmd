---
title: "Data preparation"
output:
  pdf_document: default
---

# Instructions

- You only need to submit the .Rmd of this file, not a PDF.

- You should __comment__ your code clearly to show what you've done to prepare the data.

- The purpose of this file is to use the data in the `data-raw` folder to create the data you will use in the report. The data you will use in the report should be saved in the `data` folder. It is good professional practice to make sure you're never directly modifying your raw data, but instead creating new datasets based on merges/manipulations that you need to reuse.

- Make sure you've taken a look at the hints for the web scraping and census API. 

- You may find the `write_rds()` function from the `readr` package helpful (it is loaded as part of the `tidyverse`).

- You do not need to keep the structure below.

# Set up

```{r, libraries, message=FALSE, warning=FALSE}
# Set up any libraries needed
library(tidyverse)
library(polite)
library(rvest)
library(sf)
library(geojsonsf)
library(eeptools)
library(janitor)
library(plyr)
library(dplyr)
library(cancensus)
library(haven)
```

# Loading client data

```{r load client data}
# Load all the client data from data_raw to r
break_glass_in_case_of_emergency <- readRDS("./data-raw/break_glass_in_case_of_emergency.Rds")
cust_dev <- readRDS("./data-raw/cust_dev.Rds")
cust_sleep <- readRDS("./data-raw/cust_sleep.Rds")
customer <- readRDS("./data-raw/customer.Rds")
device <- readRDS("./data-raw/device.Rds")
```

# Getting external data

## Web scraping industry data

```{r web scraping}
# Web scraping the device data from the following url
url <- "https://fitnesstrackerinfohub.netlify.app/"

# Provide the appropriate user agent for web scraping the data
target <- bow(url,
              user_agent = "xxxxxxxxx@mail.utoronto.ca for STA303/1002 project",
              force = TRUE)

# Any details provided in the robots text on crawl delays and 
# which agents are allowed to scrape
target

html <- scrape(target)

# Store the device data into r
device_data <- html %>% 
  html_elements("table") %>% 
  html_table() %>% 
  pluck(1) # added, in case you're getting a list format

```

```{r Wrangling the device data}
# Cleaning and Wrangling the device data 

# Extract the year from the released date
device_data$Released <- as.Date(device_data$Released, "%Y-%m-%d")
device_data$ReleasedYr <- format(device_data$Released, format="%Y")
device_data$ReleasedF <- as.factor(device_data$Released)


# Convert the battery life as factor
device_data$`Battery life` <- as.factor(device_data$`Battery life`)

# Mutate the level for battery life and water resitance into qppropriate order
device_data <- device_data %>% 
  mutate(`Battery life` = fct_relevel(`Battery life`, "Up to 3 days", after = 0)) %>% 
  mutate(`Battery life` = fct_relevel(`Battery life`, "Up to 5 days", after = 1)) %>%
  mutate(`Battery life` = fct_relevel(`Battery life`, "Up to 7 days", after = 2)) %>% 
  mutate(`Battery life` = fct_relevel(`Battery life`, "Up to 14 days", after = 3)) %>% 
  mutate(`Battery life` = fct_relevel(`Battery life`, "Up to 21 days", after = 4))

device_data$`Water resitance` <- as.factor(device_data$`Water resitance`)
device_data <- device_data %>% 
  mutate(`Water resitance` = fct_relevel(`Water resitance`, "Resistant", after = 0)) %>% 
  mutate(`Water resitance` = fct_relevel(`Water resitance`, "Waterproof", after = 1)) %>%
  mutate(`Water resitance` = fct_relevel(`Water resitance`, "Waterproof, 5 ATM", after = 2)) %>% 
  mutate(`Water resitance` = fct_relevel(`Water resitance`, "Waterproof, 10 ATM", after = 3))

# Set all the category variable to factors
device_data$`Heart rate sensor` <- as.factor(device_data$`Heart rate sensor`)
device_data$`Pulse oximiter` <- as.factor(device_data$`Pulse oximiter`)
device_data$GPS <- as.factor(device_data$GPS)
device_data$`Sleep tracking` <- as.factor(device_data$`Sleep tracking`)
device_data$`Smart notifications` <- as.factor(device_data$`Smart notifications`)
device_data$`Contactless payments` <- as.factor(device_data$`Contactless payments`)

# Clean the variable names of device data and write it to the data diretory
device_data <- device_data %>% janitor::clean_names()
write_csv(device_data, file = "./data/device_data.csv")

################################################################################################\
# Create the device performance dataset for convenience
device_performance <- device_data %>% 
  select(c(line, heart_rate_sensor, pulse_oximiter, gps, sleep_tracking, 
           smart_notifications, contactless_payments)) %>% 
  pivot_longer(-c(line), names_to = "name", values_to = "value") %>% 
  group_by(line, name, value) %>% 
  dplyr::summarise(n = n())

device_performance_total <- device_data %>%
  select(c(line, heart_rate_sensor, pulse_oximiter, gps, sleep_tracking, 
           smart_notifications, contactless_payments)) %>% 
  pivot_longer(-c(line), names_to = "name", values_to = "value") %>% 
  group_by(line) %>% 
  dplyr::summarise(total = n())

device_performance <- device_performance %>% 
  filter(value == "Yes")

device_performance <- merge(device_performance, device_performance_total)
device_performance$percentage <- 
  round(device_performance$n / device_performance$total, 4) *100

# write the device performance to the data directory
write_csv(device_performance, file = "./data/device_performance.csv")
```


# Census API
Used the most recent census data as most of the sleep data recorded are from 2022. We want the most up to date information s possible for the postal codes of people as they were wearing the devices. 
```{r}

dataset = read_sav("data-raw/pccfNat_fccpNat_082021sav.sav")

postcode <- dataset %>% 
  select(PC, CSDuid) %>%
  dplyr::rename(postcode = PC)

customer_with_postal <- postcode %>% right_join(customer, by = "postcode")
write.csv(postcode, "data/customer_with_postal.csv", row.names=FALSE, quote=FALSE) 
```


# Income Data

```{r Income Data}
# Use API to get the income data from the CensusMapper

# install.packages("cancensus")

# Provide the appropriate API keys and store the cache to the cache folder
options(cancensus.api_key = "CensusMapper_62d5b97e34abdba3579b1900089c724a",
        cancensus.cache_path = "cache") # this sets a folder for your cache


# get all regions as at the 2016 Census (2020 not up yet)
regions <- list_census_regions(dataset = "CA16")

regions_filtered <-  regions %>% 
  filter(level == "CSD") %>% # Figure out what CSD means in Census data
  as_census_region_list()

# This can take a while
# We want to get household median income
census_data_csd <- get_census(dataset='CA16', regions = regions_filtered,
                          vectors=c("v_CA16_2397"), 
                          level='CSD', geo_format = "sf")

# Simplify to only needed variables
median_income <- census_data_csd %>% 
  as_tibble() %>% 
  select(CSDuid = GeoUID, contains("median"), Population) %>% 
  mutate(CSDuid = parse_number(CSDuid)) %>% 
  dplyr::rename(hhld_median_inc = 2)

# Write the median_income dataset to the data directory
write_csv(median_income, file="./data/median_income.csv")

```

### demographic data

```{r demographic data}

# Join the device and customer dataset to create a more informative dataset
device_cust_id <- cust_dev %>% full_join(device, by = "dev_id")
device_cust <-device_cust_id %>% full_join(customer, by = "cust_id")

# Extract the year from the date, and create the age column based on the date
device_cust$dob <- as.Date(device_cust$dob)
device_cust$age <- floor(age_calc(device_cust$dob, units = "years"))

# Join the dataset with itself by postcode
device_cust <- unique(inner_join(device_cust, postcode, by="postcode") )
```


### "racist data"
```{r racist data}
# Wrangling the racist dataset to investigate the our second research question

# Filter and select all the interested fields
race_sleep_data <- customer %>% 
  full_join(cust_sleep, by = "cust_id") %>% 
  select("cust_id", "sex", "emoji_modifier", "date", "duration", "flags") %>%
  filter(!is.na(date)) %>%
  filter(!is.na(duration)) %>%
  filter(!is.na(flags)) %>%
  filter(!is.na(emoji_modifier))

# Created new column for skin tone based on the emoji modifier in the dataset
race_sleep_data$skin_tone <- as.factor(
      ifelse(race_sleep_data$emoji_modifier == "U+1F3FB","light", 
      ifelse(race_sleep_data$emoji_modifier == "U+1F3FC", "medium_light", 
      ifelse(race_sleep_data$emoji_modifier == "U+1F3FD", "medium", 
      ifelse(race_sleep_data$emoji_modifier == "U+1F3FE", "medium_dark", "dark")))))

# Join the race data with outher dataset to create a more informative dataset
race_sleep_data <- race_sleep_data %>% 
  left_join(cust_dev, by = "cust_id") 

race_sleep_data <- race_sleep_data %>% 
  left_join(device, by = "dev_id") %>% 
  select("cust_id", "sex", "skin_tone", "date", 
         "duration", "flags", "dev_id", "device_name")

# Clean the variable names 
device_data <- device_data %>% janitor::clean_names() 

race_sleep_data <- race_sleep_data %>% 
  left_join(device_data, by = "device_name") %>% 
  select("cust_id", "sex", "skin_tone", "date", "duration", 
         "flags", "device_name", "line", "battery_life", 
         "water_resitance", "heart_rate_sensor", "pulse_oximiter", 
         "gps", "sleep_tracking", "smart_notifications")

#put in data folder
write_csv(race_sleep_data, file = "data/race_sleep_data.csv")
```






